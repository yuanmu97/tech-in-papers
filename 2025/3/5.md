# Agentic AI

https://blogs.nvidia.com/blog/what-is-agentic-ai/

在 chatbot 完成问答能力之上的、具有一定自主性的 AI。NV 举的例子是：用于客户服务的 AI agent 可以超越简单的问题解答，它可以检查用户的欠款余额，并推荐哪些账户可以还清欠款，同时等待用户做出决定，以便在给出 prompt 时完成相应的交易。

Agentic AI 通过**四步**解决问题：

1. 感知 perceive：从多种数据源，包括传感器、数据库等，获取并处理数据。有点像具身智能的概念。
2. 推理 reason：使用 LLMs 理解任务、生成规划、协调特定任务模型（例如内容创造、视觉处理和推荐系统）、使用 RAG 来获取私域数据等等。
3. 行动 act：通过 API 与外部工具和软件集成，agent 可以根据自己制定的计划快速执行任务。可以在 agent 中内置防护栏，以帮助确保其正确执行任务。例如，客户服务人工智能代理可以处理一定金额以内的索赔，而超过该金额的索赔则必须由人工批准。
4. 学习 learn：agent 通过反馈回路不断改进，或者说"数据飞轮 flywheel"，将互动产生的数据输入系统，以增强模型。随着时间的推移，这种适应和提高效率的能力为企业提供了一个强大的工具，以推动更好的决策和运营效率。

> [!TIP]
>
> 这里我觉得和 AI Agent 的概念基本上是一致的。值得研究的点可能有：
>
> 1. 如何真正实现 modality-native 的大模型？而非现在通过 projection + concatenation 的方式接入 LLM 实现多模态。
> 2. 如果未来的 Agent 完全通用化，那么其推理过程一定是需要模块化的。类比于 OS，足够的通用，能够支持各种上层应用，但是并不会每个应用都需要用到 OS 的全部组件。也就是说，在 inference 阶段，要能足够智能化地利用 Agent 的稀疏性。可能不会是确定性的模块化，应该是 nondeterministic modules 的感觉。解决了这个问题，才有可能真正用一个模型统一处理各种 reason 任务。
> 3. 这块 NV blog 里提到的“防护栏 Guardrails”是偏 AI 内容安全的。这一点感觉对于具身智能更加重要，因为其可以执行的 action 可能具有对人的物理上的伤害性。有没有方法从理论上解决这个问题？
> 4. 数据飞轮，以往可能是终身学习的概念，对于生成式模型，存在的问题有：如何筛选“合适”的数据（包括真实的和合成的）进行模型更新？模型-to-模型的数据交换，是否可以更有效率？

